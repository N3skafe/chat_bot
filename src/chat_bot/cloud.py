import os
import chromadb
import requests
import json
import time
import gradio as gr
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

class SimpleRAG:
    def __init__(
        self, 
        collection_name: str = "documents", 
        embedding_model_name: str = "all-MiniLM-L6-v2",
        chroma_db_path: str = "./chroma_db",
        ollama_base_url: str = "http://localhost:11434"
    ):
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print(f"ì„ë² ë”© ëª¨ë¸ '{embedding_model_name}' ë¡œë”© ì¤‘...")
        self.embedding_model = SentenceTransformer(embedding_model_name)
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        print(f"ChromaDB ì´ˆê¸°í™” ì¤‘ (ê²½ë¡œ: {chroma_db_path})...")
        self.db_path = chroma_db_path
        os.makedirs(self.db_path, exist_ok=True)
        self.client = chromadb.PersistentClient(path=self.db_path)
        
        # Ollama ê¸°ë³¸ URL ì„¤ì •
        self.ollama_base_url = ollama_base_url
        
        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        try:
            self.collection = self.client.get_collection(collection_name)
            print(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}'ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. ë¬¸ì„œ ìˆ˜: {self.collection.count()}")
        except Exception as e:
            print(f"ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ '{collection_name}'ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"hnsw:space": "cosine"}
            )
    
    def add_documents(
        self,
        documents: List[str],
        ids: Optional[List[str]] = None,
        metadatas: Optional[List[Dict[str, Any]]] = None,
        batch_size: int = 32
    ):
        """ë¬¸ì„œë¥¼ ChromaDBì— ì¶”ê°€"""
        # ë¬¸ì„œ ID ìƒì„±
        if ids is None:
            start_idx = self.collection.count()
            ids = [f"doc_{start_idx + i}" for i in range(len(documents))]
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        if metadatas is None:
            metadatas = [{"source": "unknown"} for _ in range(len(documents))]
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        total_docs = len(documents)
        for i in range(0, total_docs, batch_size):
            end_idx = min(i + batch_size, total_docs)
            batch_texts = documents[i:end_idx]
            batch_ids = ids[i:end_idx]
            batch_metadatas = metadatas[i:end_idx]
            
            # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
            batch_embeddings = self.embedding_model.encode(batch_texts).tolist()
            
            # ì»¬ë ‰ì…˜ì— ë¬¸ì„œ ì¶”ê°€
            self.collection.add(
                documents=batch_texts,
                embeddings=batch_embeddings,
                ids=batch_ids,
                metadatas=batch_metadatas
            )
        
        print(f"ì´ {total_docs}ê°œì˜ ë¬¸ì„œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return f"ì´ {total_docs}ê°œì˜ ë¬¸ì„œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    def query_documents(self, query: str, n_results: int = 3):
        """ì¿¼ë¦¬ì— ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.embedding_model.encode(query).tolist()
        
        # ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        return results
    
    def query_ollama(self, prompt: str, model: str = "deepseek-r1:latest"):
        """Ollama APIë¥¼ í†µí•´ LLMì— ì¿¼ë¦¬ ì „ì†¡"""
        url = f"{self.ollama_base_url}/api/generate"
        data = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        
        try:
            response = requests.post(url, json=data, timeout=30)
            response.raise_for_status()
            return response.json()["response"]
        except requests.exceptions.RequestException as e:
            print(f"Ollama API ì˜¤ë¥˜: {e}")
            return f"ì˜¤ë¥˜: Ollama API ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
    
    def rag_query(self, user_query: str, n_results: int = 3, model: str = "deepseek-r1:latest"):
        """RAG íŒŒì´í”„ë¼ì¸ - ì¿¼ë¦¬, ê²€ìƒ‰, LLM ì‘ë‹µ ìƒì„±"""
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        search_results = self.query_documents(user_query, n_results)
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ë‹¤ë©´
        if not search_results["documents"][0]:
            prompt = f"ì§ˆë¬¸: {user_query}\n\nê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•Œê³  ìˆëŠ” ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."
        else:
            # 2. ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            
            for i, doc in enumerate(search_results["documents"][0]):
                context_parts.append(f"[ë¬¸ì„œ {i+1}]\n{doc}")
            
            context = "\n\n".join(context_parts)
            
            # 3. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

### ì°¸ê³  ì •ë³´:
{context}

### ì§ˆë¬¸:
{user_query}

### ë‹µë³€:
"""
        
        # 4. Ollama ëª¨ë¸ì— ì¿¼ë¦¬ ì „ì†¡
        response = self.query_ollama(prompt, model)
        
        return response, search_results

    def process_file(self, file_obj, chunk_size=1000, overlap=200):
        """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì²­í¬ë¡œ ë¶„í• """
        try:
            text = file_obj.decode('utf-8')
            chunks = self._split_text_into_chunks(text, chunk_size, overlap)
            
            # ì²­í¬ë¥¼ ë¬¸ì„œë¡œ ì¶”ê°€
            metadatas = [{"source": "uploaded_file", "chunk_id": i} for i in range(len(chunks))]
            result = self.add_documents(chunks, metadatas=metadatas)
            return result
        except Exception as e:
            return f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def _split_text_into_chunks(self, text, chunk_size, overlap):
        """í…ìŠ¤íŠ¸ë¥¼ ê²¹ì¹˜ëŠ” ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            end = min(start + chunk_size, text_len)
            
            # ë‹¨ì–´ ê²½ê³„ì—ì„œ ë¶„í• 
            if end < text_len:
                while end > start and not text[end].isspace():
                    end -= 1
                if end == start:  # ì ì ˆí•œ ê³µë°±ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
                    end = min(start + chunk_size, text_len)
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ë‹¤ìŒ ì²­í¬ì˜ ì‹œì‘ ìœ„ì¹˜ (ê²¹ì¹¨ ê³ ë ¤)
            start = max(end - overlap, start + 1)
        
        return chunks

# ìƒ˜í”Œ ë¬¸ì„œ ì´ˆê¸°í™” í•¨ìˆ˜
def init_sample_documents(rag_system):
    if rag_system.collection.count() == 0:
        print("ìƒ˜í”Œ ë¬¸ì„œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤...")
        sample_documents = [
            "RAG(Retrieval-Augmented Generation)ëŠ” LLMì˜ í™˜ê° í˜„ìƒì„ ì¤„ì´ê³  ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.",
            "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.",
            "Chroma DBëŠ” íŒŒì´ì¬ì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.",
            "ì„ë² ë”© ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.",
            "DeepSeekëŠ” ìµœê·¼ ê³µê°œëœ ì˜¤í”ˆì†ŒìŠ¤ LLM ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ, ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "ë²¡í„° ê²€ìƒ‰ì€ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ê¸° ìœ„í•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ê°™ì€ ê±°ë¦¬ ì¸¡ì • ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
            "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì€ LLMì—ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
            "OllamaëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ LLMì„ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤."
        ]
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        metadatas = [{"source": "rag_info", "category": "ai"} for _ in range(len(sample_documents))]
        
        # ë¬¸ì„œ ì¶”ê°€
        rag_system.add_documents(sample_documents, metadatas=metadatas)
        return f"ìƒ˜í”Œ ë¬¸ì„œ {len(sample_documents)}ê°œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    return f"ê¸°ì¡´ ì»¬ë ‰ì…˜ì— {rag_system.collection.count()}ê°œì˜ ë¬¸ì„œê°€ ìˆìŠµë‹ˆë‹¤."

# Gradio ì¸í„°í˜ì´ìŠ¤ì™€ ì—°ê²°í•  í•¨ìˆ˜ë“¤
def ask_question(query, model_name, num_results, rag_system):
    if not query.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""
    
    start_time = time.time()
    response, search_results = rag_system.rag_query(
        query, 
        n_results=num_results, 
        model=model_name
    )
    end_time = time.time()
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
    retrieved_docs = ""
    if search_results["documents"][0]:
        for i, (doc, metadata) in enumerate(zip(search_results["documents"][0], search_results["metadatas"][0])):
            source = metadata.get("source", "unknown")
            retrieved_docs += f"ğŸ“„ ë¬¸ì„œ {i+1} (ì¶œì²˜: {source}):\n{doc}\n\n"
    else:
        retrieved_docs = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    # ì‘ë‹µì— ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
    full_response = f"{response}\n\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ"
    
    return full_response, retrieved_docs

def upload_file(files, chunk_size, chunk_overlap, rag_system):
    if not files:
        return "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
    
    results = []
    for file in files:
        result = rag_system.process_file(file, int(chunk_size), int(chunk_overlap))
        results.append(f"íŒŒì¼ '{os.path.basename(file.name)}': {result}")
    
    return "\n".join(results)

def add_text_document(text, rag_system):
    if not text.strip():
        return "ë¬¸ì„œ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    rag_system.add_documents([text])
    return f"ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ì´ {rag_system.collection.count()}ê°œì˜ ë¬¸ì„œê°€ ìˆìŠµë‹ˆë‹¤."

def clear_chat_history():
    return "", ""

def main():
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = SimpleRAG()
    
    # ìƒ˜í”Œ ë¬¸ì„œ ì´ˆê¸°í™”
    init_message = init_sample_documents(rag_system)
    
    # Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
    with gr.Blocks(title="RAG ì‹œìŠ¤í…œ - ChromaDB + Ollama") as demo:
        gr.Markdown("# ğŸ“š RAG ì‹œìŠ¤í…œ (ChromaDB + Ollama + DeepSeek)")
        gr.Markdown(f"### í˜„ì¬ ë¬¸ì„œ ìˆ˜: {rag_system.collection.count()}")
        
        with gr.Tabs():
            # ì§ˆë¬¸ íƒ­
            with gr.TabItem("ì§ˆë¬¸í•˜ê¸°"):
                with gr.Row():
                    with gr.Column(scale=2):
                        query_input = gr.Textbox(
                            label="ì§ˆë¬¸",
                            placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                            lines=2
                        )
                        with gr.Row():
                            model_select = gr.Dropdown(
                                label="ëª¨ë¸",
                                choices=["deepseek-r1:latest", "llama3:latest", "phi3:latest"],
                                value="deepseek-r1:latest"
                            )
                            num_results = gr.Slider(
                                label="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜",
                                minimum=1,
                                maximum=10,
                                step=1,
                                value=3
                            )
                        
                        with gr.Row():
                            ask_button = gr.Button("ì§ˆë¬¸í•˜ê¸°")
                            clear_button = gr.Button("ëŒ€í™” ì§€ìš°ê¸°")
                    
                with gr.Row():
                    with gr.Column():
                        answer_output = gr.Textbox(label="ë‹µë³€", lines=10)
                    with gr.Column():
                        docs_output = gr.Textbox(label="ê²€ìƒ‰ëœ ë¬¸ì„œ", lines=10)
            
            # ë¬¸ì„œ ì¶”ê°€ íƒ­
            with gr.TabItem("ë¬¸ì„œ ì¶”ê°€"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### íŒŒì¼ ì—…ë¡œë“œ")
                        file_input = gr.File(label="íŒŒì¼ ì„ íƒ", file_types=[".txt", ".md", ".csv"], file_count="multiple")
                        with gr.Row():
                            chunk_size = gr.Number(label="ì²­í¬ í¬ê¸°", value=1000)
                            chunk_overlap = gr.Number(label="ì²­í¬ ê²¹ì¹¨", value=200)
                        upload_button = gr.Button("íŒŒì¼ ì—…ë¡œë“œ")
                    
                    with gr.Column():
                        gr.Markdown("### í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥")
                        text_input = gr.Textbox(label="ë¬¸ì„œ ë‚´ìš©", lines=10, placeholder="ì—¬ê¸°ì— ë¬¸ì„œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”...")
                        add_text_button = gr.Button("í…ìŠ¤íŠ¸ ì¶”ê°€")
                
                upload_result = gr.Textbox(label="ê²°ê³¼")
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        ask_button.click(
            fn=lambda q, m, n: ask_question(q, m, n, rag_system),
            inputs=[query_input, model_select, num_results],
            outputs=[answer_output, docs_output]
        )
        
        clear_button.click(
            fn=clear_chat_history,
            inputs=[],
            outputs=[answer_output, docs_output]
        )
        
        upload_button.click(
            fn=lambda f, cs, co: upload_file(f, cs, co, rag_system),
            inputs=[file_input, chunk_size, chunk_overlap],
            outputs=upload_result
        )
        
        add_text_button.click(
            fn=lambda t: add_text_document(t, rag_system),
            inputs=[text_input],
            outputs=upload_result
        )
    
    # Gradio ì•± ì‹¤í–‰
    demo.launch(share=False)

if __name__ == "__main__":
    main()