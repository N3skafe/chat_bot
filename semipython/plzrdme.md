Ollama의 오픈소스 LLM 모델(deepseek-r1, llama3.2, gemma)을 활용한 멀티 에이전트 RAG 시스템입니다. 시스템의 주요 기능과 구성요소는 다음과 같습니다:

모델 시스템:
각 모델 유형에 따라 적합한 태스크 수행 (코드: deepseek, 복잡한 추론: llama3, 일반 질문: gemma)
Ollama API를 통해 모델과 통신
벡터 스토어:
PDF 파일 처리 및 텍스트 추출
Chroma DB를 이용한 벡터 저장소 구현
의미론적 검색 기능
RAG 구현:
쿼리에 관련된 문서 검색
검색된 문서를 컨텍스트로 활용하여 응답 생성
멀티 에이전트 시스템:
LangGraph를 사용한 워크플로우 구현
단계별 처리 (라우팅, 검색, RAG, 심층 연구, 요약)
쿼리 유형에 따른 적절한 모델 자동 선택
Gradio 인터페이스:
사용자 친화적 채팅 인터페이스
PDF 업로드 및 처리 기능
이 시스템은 질문 유형에 따라 적절한 모델을 자동으로 선택하고, RAG를 통해 외부 지식을 활용하며, 여러 모델의 장점을 조합하여 보다 정확하고 포괄적인 응답을 생성합니다.

사용하려면 Ollama를 설치하고 필요한 모델(deepseek-r1, llama3.2, gemma)을 다운로드한 후, Poetry를 통해 의존성을 설치하고 main.py를 실행하면 됩니다.
